{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "import functools\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we generate all possible symmetry board of tic tac toe\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]\n",
    "MAGIC_90=[6,1,8,7,5,3,2,9,4]\n",
    "MAGIC_180=[8,3,4,1,5,9,6,7,2]\n",
    "MAGIC_270=[4,9,2,3,5,7,8,1,6]\n",
    "MAGIC_MIRROR=[4,3,8,9,5,1,2,7,6]\n",
    "MAGIC_MIRROR_90=[2,9,4,7,5,3,6,1,8]\n",
    "MAGIC_MIRROR_180=[6,7,2,1,5,9,8,3,4]\n",
    "MAGIC_MIRROR_270=[8,1,6,3,5,7,4,9,2]\n",
    "\n",
    "MAGIC_BOARDS = [MAGIC, MAGIC_90, MAGIC_180, MAGIC_270, MAGIC_MIRROR, MAGIC_MIRROR_90, MAGIC_MIRROR_180, MAGIC_MIRROR_270]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We define this 'compare' function to sort all possible configuration of one state in order to convert it in a common configuration\n",
    "This function compare two states in this way: \n",
    "    - First it compare the elements of self.x one by one with the elements of other.x, if the two element are the same it compare the next element of 'x' and so on\n",
    "    - If all element of 'x' are the same it do the same thing with 'o'\n",
    "'''\n",
    "\n",
    "def compare(self, other):\n",
    "    \n",
    "    for elem_self, elem_other in zip(self.x, other.x):\n",
    "        if elem_self < elem_other:\n",
    "            return -1\n",
    "        elif elem_self > elem_other:\n",
    "            return 1\n",
    "\n",
    "    for elem_self, elem_other in zip(self.o, other.o):\n",
    "        if elem_self < elem_other:\n",
    "            return -1\n",
    "        elif elem_self > elem_other:\n",
    "            return 1\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indexes(state:State):\n",
    "        #This function extract positional index of state's element in the 'MAGIC' bord\n",
    "        state_indexes = State(set(), set())\n",
    "\n",
    "        for val in state.x:\n",
    "                index = MAGIC.index(val)\n",
    "                state_indexes.x.add(index)\n",
    "\n",
    "        for val in state.o:\n",
    "                index = MAGIC.index(val)\n",
    "                state_indexes.o.add(index)\n",
    "        \n",
    "        return state_indexes\n",
    "         \n",
    "def get_representation_by_index(state_indexes:State, magic):\n",
    "    #Given state indexes in the 'MAGIC' board, \n",
    "    #this function return the representation of the state in another variant of magic board passed as parameter\n",
    "    result= State(set(),set())\n",
    "\n",
    "    for i in state_indexes.x:\n",
    "        value = magic[i]\n",
    "        result.x.add(value)\n",
    "\n",
    "    for i in state_indexes.o:\n",
    "        value = magic[i]\n",
    "        result.o.add(value) \n",
    "\n",
    "    return result\n",
    "        \n",
    "def get_equivalent_representations(state:State):\n",
    "    #Given a state this function returns a list containing all equivalent state representation in the different magic boards\n",
    "    representations = []\n",
    "    state_indexes = find_indexes(state)\n",
    "\n",
    "    for magic in MAGIC_BOARDS:\n",
    "        representations.append(get_representation_by_index(state_indexes, magic))\n",
    "\n",
    "    return representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3553939072360903781\n",
      "-3553939072360903781\n"
     ]
    }
   ],
   "source": [
    "class CustomState(namedtuple('State', ['x', 'o'])):\n",
    "    #for the hash we generate all possible configuration from one state and then we re order the vector with comapre fucntion and extract first element\n",
    "    #in this way we have always the same configuration\n",
    "\n",
    "    def __eq__(self, other_state):\n",
    "        #To see if one representation is equivalent to another one we extract indices in the 'MAGIC' board \n",
    "        #and then we search those indexes in all magic boards to extract the equivalent representations\n",
    "        #If one is equivalent to other_state, the two state are the same\n",
    "\n",
    "        state_indexes = find_indexes(self)\n",
    "        for magic in MAGIC_BOARDS:\n",
    "            representation = get_representation_by_index(state_indexes, magic)\n",
    "            if (sorted(representation.x) == sorted(other_state.x) and sorted(representation.o) == sorted(other_state.o)):\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        #We generate all possible equivalent representations from one state\n",
    "        #and then we re-order them with 'compare' function, we extract first element \n",
    "        #and then we apply hash functio to its string representation\n",
    "\n",
    "        rappresentations = get_equivalent_representations(self)\n",
    "        sorted_rappresentations = sorted(rappresentations, key=functools.cmp_to_key(compare))\n",
    "\n",
    "        return hash(str(sorted_rappresentations))\n",
    "    \n",
    "    def unique_representation(self):\n",
    "        #This function convert the state in its unique representation         \n",
    "        rappresentations = get_equivalent_representations(self)\n",
    "        sorted_rappresentations = sorted(rappresentations, key=functools.cmp_to_key(compare))\n",
    "            \n",
    "        return CustomState(sorted_rappresentations[0].x, sorted_rappresentations[0].o)\n",
    "\n",
    "print(CustomState(set([1]),set([4])).__hash__())\n",
    "\n",
    "print(CustomState(set([1]),set([2])).__hash__())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning  DA MODIFICARE\n",
    "The reward are:\n",
    "- 1 for win\n",
    "- 0.75 if we block adversarial win\n",
    "- 0.5 if we make a trap\n",
    "and viceversa but negative for adversarial\n",
    "\n",
    "We've implemented the symmetry for board of tic tac toe, in this way instead of save all possible configuration we save only one configuration, we chose the common configuration with reverse function (from a state we generate all possible configuration and take the first after that we re order the configuration using compare function for key). In this way we pass from a dictionary of 5k element (da verificare) to a dictionary of 1k element (da verificare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_move(available):\n",
    "    x = choice(list(available))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    #Checks is elements is winning\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def block_win_adv(adv_elements, action):\n",
    "    #Checks if our action can block adversarial win\n",
    "    for c in combinations(adv_elements, 2):\n",
    "        if 15 - sum(c) == action:\n",
    "            return True\n",
    "    return False\n",
    " \n",
    "def trap_condition(user_elements, adv_elements):\n",
    "    #Checks if the user successfully create a double trap condition for the advesary in the given state \n",
    "    cnt = 0\n",
    "    for c in combinations(user_elements, 2):\n",
    "        val = 15 - sum(c)\n",
    "        if val not in adv_elements and val > 0:\n",
    "            cnt += 1\n",
    "            if cnt >= 2:\n",
    "                return True         \n",
    "    return False\n",
    " \n",
    "def state_value(state: State):\n",
    "    #Evaluate state: +1 first player wins\n",
    "    if win(state.x):\n",
    "        return 1\n",
    "    elif win(state.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(steps, learning_rate, discount_factor):\n",
    "    value_dictionary = {}\n",
    "\n",
    "    for _ in tqdm(range(steps)):\n",
    "        current_state = CustomState(set(), set())\n",
    "        cnt = 0\n",
    "\n",
    "        while current_state.x.union(current_state.o) != set(range(1, 10)) and state_value(current_state) == 0:\n",
    "            next_state = deepcopy(current_state)\n",
    "            current_state = current_state.unique_representation()\n",
    "            next_state = next_state.unique_representation()\n",
    "            action = random_move(set(range(1, 10)) - (current_state.x.union(current_state.o)))\n",
    "            player = cnt % 2\n",
    "            cnt += 1\n",
    "\n",
    "            if player == 1:\n",
    "                next_state.x.add(action)\n",
    "                next_state = next_state.unique_representation()\n",
    "                reward = state_value(next_state)\n",
    "\n",
    "                if(reward == 0):\n",
    "                    if block_win_adv(next_state.o, action):\n",
    "                        reward = 0.75\n",
    "                    elif trap_condition(next_state.x, next_state.o):\n",
    "                        reward = 0.5\n",
    "\n",
    "                if current_state not in value_dictionary:\n",
    "                    value_dictionary[current_state] = {action: 0.}\n",
    "                elif action not in value_dictionary[current_state]:\n",
    "                    value_dictionary[current_state][action] = 0.\n",
    "\n",
    "                if next_state not in value_dictionary:\n",
    "                    value_dictionary[next_state] = {action: 0.}\n",
    "                elif action not in value_dictionary[next_state]:\n",
    "                    value_dictionary[next_state][action] = 0.\n",
    "\n",
    "                value_dictionary[current_state][action] = ((1 - learning_rate) * value_dictionary[current_state][action] + \n",
    "                    learning_rate * (reward + discount_factor * max(value_dictionary[next_state].values())))\n",
    "                current_state = deepcopy(next_state)\n",
    "\n",
    "            else:  \n",
    "                next_state.o.add(action)\n",
    "                next_state = next_state.unique_representation()\n",
    "                reward = state_value(next_state)\n",
    "\n",
    "                if(reward == 0):\n",
    "                    if block_win_adv(next_state.x, action):\n",
    "                        reward = -0.75\n",
    "                    elif trap_condition(next_state.o, next_state.x):\n",
    "                        reward = -0.5\n",
    "\n",
    "                if current_state not in value_dictionary:\n",
    "                    value_dictionary[current_state] = {action: 0.}\n",
    "                elif action not in value_dictionary[current_state]:\n",
    "                    value_dictionary[current_state][action] = 0.\n",
    "\n",
    "                if next_state not in value_dictionary:\n",
    "                    value_dictionary[next_state] = {action: 0.}\n",
    "                elif action not in value_dictionary[next_state]:\n",
    "                    value_dictionary[next_state][action] = 0.\n",
    "\n",
    "                value_dictionary[current_state][action] = ((1 - learning_rate) * value_dictionary[current_state][action] + \n",
    "                    learning_rate * (reward + discount_factor * min(value_dictionary[next_state].values())))\n",
    "                current_state = deepcopy(next_state)\n",
    "\n",
    "    return value_dictionary       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stampa_dizionario(dizionario, livello=0):\n",
    "    spazi = \"  \" * livello\n",
    "    for chiave, valore in dizionario.items():\n",
    "        if isinstance(valore, dict):\n",
    "            print(f\"{spazi}{chiave}:\")\n",
    "            stampa_dizionario(valore, livello + 1)\n",
    "        else:\n",
    "            print(f\"{spazi}{chiave}: {valore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_against_random(value_dictionary, invert=False):\n",
    "    current_state = CustomState(set(),set())\n",
    "\n",
    "    while len(current_state.x.union(current_state.o)) < 9 and state_value(current_state) == 0:\n",
    "        if invert:\n",
    "            ## Player 1 (random agent)\n",
    "            action = random_move(set(range(1, 10)) - (current_state.x.union(current_state.o)))\n",
    "            current_state.o.add(action)\n",
    "            current_state = current_state.unique_representation()\n",
    "\n",
    "            if len(current_state.x.union(current_state.o)) == 9 or state_value(current_state) == -1:\n",
    "                break\n",
    "\n",
    "            ## Player 2 (RL agent)\n",
    "            list_action = sorted(value_dictionary[current_state], key=value_dictionary[current_state].get)\n",
    "            \n",
    "            for action in list_action:\n",
    "                if action not in (current_state.x.union(current_state.o)):\n",
    "                    current_state.x.add(action)\n",
    "                    break\n",
    "        else:\n",
    "            ## Player 1 (RL agent)\n",
    "            list_action = sorted(value_dictionary[current_state], key=value_dictionary[current_state].get)\n",
    "            \n",
    "            for action in list_action:\n",
    "                if action not in (current_state.x.union(current_state.o)):\n",
    "                    current_state.x.add(action)\n",
    "                    break\n",
    "            \n",
    "            if len(current_state.x.union(current_state.o)) == 9 or state_value(current_state) == 1:\n",
    "                break\n",
    "            \n",
    "            ## Player 2 (random agent)\n",
    "            action = random_move(set(range(1, 10)) - (current_state.x.union(current_state.o)))\n",
    "            current_state.o.add(action)\n",
    "\n",
    "        current_state = current_state.unique_representation()\n",
    "    \n",
    "    return state_value(current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99d462cb37a47c0abc99956cd4e91f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- learning rate: 0.10     discount factor: 0.50     win rate: 48.50%\n",
      "n_lose: 96    n_draw: 7, n_win: 97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4afdc9d2e0d48da9685d848de68080c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- learning rate: 0.10     discount factor: 0.70     win rate: 51.00%\n",
      "n_lose: 86    n_draw: 12, n_win: 102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b5691111f2410d93af773cbad41e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- learning rate: 0.30     discount factor: 0.50     win rate: 38.50%\n",
      "n_lose: 91    n_draw: 32, n_win: 77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc6bd6c336842268bb309c57638a09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- learning rate: 0.30     discount factor: 0.70     win rate: 48.50%\n",
      "n_lose: 92    n_draw: 11, n_win: 97\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "lr_params = [0.1, 0.3]\n",
    "df_params = [0.5, 0.7]\n",
    "\n",
    "print('Results:\\n')\n",
    "for lr in lr_params:\n",
    "    for df in df_params:\n",
    "        value_dictionary = q_learning(steps, lr, df)\n",
    "        \n",
    "        n_win = 0\n",
    "        n_draw = 0\n",
    "        n_lose = 0\n",
    "\n",
    "        for _ in range(100):\n",
    "            result = play_against_random(value_dictionary)\n",
    "            if result == 1:\n",
    "                n_win += 1\n",
    "            elif result == -1:\n",
    "                n_lose += 1\n",
    "            else:\n",
    "                n_draw += 1\n",
    "        \n",
    "        for _ in range(100):\n",
    "            result = play_against_random(value_dictionary, invert=True)\n",
    "            if result == 1:\n",
    "                n_win += 1\n",
    "            elif result == -1:\n",
    "                n_lose += 1\n",
    "            else:\n",
    "                n_draw += 1\n",
    "        \n",
    "        win_rate = n_win / (n_win + n_draw + n_lose)\n",
    "        print('\\t- learning rate: {0:.2f}     discount factor: {1:.2f}     win rate: {2:.2%}'.format(lr, df, win_rate))\n",
    "        print('n_lose: {0}    n_draw: {1}, n_win: {2}'.format(n_lose, n_draw, n_win))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9984688bd9e4062b3f2ce7327158288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0:\n",
      "Win: 63\tLose: 10\tDraw: 27\n",
      "Test 1:\n",
      "Win: 6\tLose: 90\tDraw: 4\n"
     ]
    }
   ],
   "source": [
    "value_dictionary = q_learning(10000, 0.1, 0.7)\n",
    "for i in range(2):\n",
    "    n_win = 0\n",
    "    n_draw = 0\n",
    "    n_lose = 0\n",
    "    print('Test {0}:'.format(i))\n",
    "    for _ in range(100):\n",
    "        if i == 0:\n",
    "            result = play_against_random(value_dictionary)\n",
    "        else:\n",
    "            result = play_against_random(value_dictionary, invert=True)\n",
    "\n",
    "        if result == 1:\n",
    "            n_win += 1\n",
    "        elif result == -1:\n",
    "            n_lose += 1\n",
    "        else:\n",
    "            n_draw += 1\n",
    "    print('Win: {0}\\tLose: {1}\\tDraw: {2}'.format(n_win, n_lose, n_draw))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
